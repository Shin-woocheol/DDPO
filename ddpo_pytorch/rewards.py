from PIL import Image
import io
import numpy as np
import torch
import hpsv2
from hpsv2.src.open_clip import create_model_and_transforms, get_tokenizer
import torchvision


def jpeg_incompressibility():
    def _fn(images, prompts, metadata):
        if isinstance(images, torch.Tensor):
            images = (images * 255).round().clamp(0, 255).to(torch.uint8).cpu().numpy()
            images = images.transpose(0, 2, 3, 1)  # NCHW -> NHWC
        images = [Image.fromarray(image) for image in images]
        buffers = [io.BytesIO() for _ in images]
        for image, buffer in zip(images, buffers):
            image.save(buffer, format="JPEG", quality=95)
        sizes = [buffer.tell() / 1000 for buffer in buffers]
        return np.array(sizes), {}

    return _fn


def jpeg_compressibility():
    #* 함수 return을 통해서, incompressibility함수를 그대로 사용하면서 -reward로만 해줌.
    jpeg_fn = jpeg_incompressibility() #* 이렇게 function을 init할수도 잇네. function을 return하니까.

    def _fn(images, prompts, metadata):
        rew, meta = jpeg_fn(images, prompts, metadata)
        return -rew, meta

    return _fn


def aesthetic_score():
    from ddpo_pytorch.aesthetic_scorer import AestheticScorer

    scorer = AestheticScorer(dtype=torch.float32).cuda()

    def _fn(images, prompts, metadata):
        if isinstance(images, torch.Tensor):
            images = (images * 255).round().clamp(0, 255).to(torch.uint8)
        else:
            images = images.transpose(0, 3, 1, 2)  # NHWC -> NCHW
            images = torch.tensor(images, dtype=torch.uint8)
        scores = scorer(images)
        return scores, {}

    return _fn

def hps(inference_dtype, device):
    from .hpsv2_scorer import HPSv2Scorer

    scorer = HPSv2Scorer(dtype=torch.float32, device=device)
    scorer.requires_grad_(False)

    def _fn(images, prompts, metadata):
        scores = scorer(images, prompts)
        return scores, {}

    return _fn



# def hps(inference_dtype=None, device=None):
#     model_name = "ViT-H-14"
#     model, preprocess_train, preprocess_val = create_model_and_transforms(
#         model_name,
#         'laion2B-s32B-b79K',
#         precision=inference_dtype,
#         device=device,
#         jit=False,
#         force_quick_gelu=False,
#         force_custom_text=False,
#         force_patch_dropout=False,
#         force_image_size=None,
#         pretrained_image=False,
#         image_mean=None,
#         image_std=None,
#         light_augmentation=True,
#         aug_cfg={},
#         output_dict=True,
#         with_score_predictor=False,
#         with_region_predictor=False
#     )    
    
#     tokenizer = get_tokenizer(model_name)
    
#     link = "https://huggingface.co/spaces/xswu/HPSv2/resolve/main/HPS_v2_compressed.pt"
#     import os
#     import requests
#     from tqdm import tqdm

#     # Create the directory if it doesn't exist
#     os.makedirs(os.path.expanduser('~/.cache/hpsv2'), exist_ok=True)
#     checkpoint_path = f"{os.path.expanduser('~')}/.cache/hpsv2/HPS_v2_compressed.pt"

#     # Download the file if it doesn't exist
#     if not os.path.exists(checkpoint_path):
#         response = requests.get(link, stream=True)
#         total_size = int(response.headers.get('content-length', 0))

#         with open(checkpoint_path, 'wb') as file, tqdm(
#             desc="Downloading HPS_v2_compressed.pt",
#             total=total_size,
#             unit='iB',
#             unit_scale=True,
#             unit_divisor=1024,
#         ) as progress_bar:
#             for data in response.iter_content(chunk_size=1024):
#                 size = file.write(data)
#                 progress_bar.update(size)
    
    
#     # force download of model via score
#     hpsv2.score([], "")
    
#     checkpoint = torch.load(checkpoint_path, map_location=device)
#     model.load_state_dict(checkpoint['state_dict'])
#     tokenizer = get_tokenizer(model_name)
#     # # 기본 dtype이 None이면 float32로 강제
#     # if inference_dtype is None:
#     #     inference_dtype = torch.float32
#     model = model.to(device, dtype=inference_dtype)
#     model.eval()

#     target_size =  224
#     normalize = torchvision.transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],
#                                                 std=[0.26862954, 0.26130258, 0.27577711])
        
#     def _fn(im_pix, prompts, metadata=None):    
#         im_pix = ((im_pix / 2) + 0.5).clamp(0, 1) 
#         x_var = torchvision.transforms.Resize(target_size)(im_pix)
#         x_var = normalize(x_var).to(im_pix.dtype)        
#         caption = tokenizer(prompts)
#         caption = caption.to(device)
#         outputs = model(x_var, caption)
#         image_features, text_features = outputs["image_features"], outputs["text_features"]
#         logits = image_features @ text_features.T

#         scores = torch.diagonal(logits)
#         return scores, {}
    
#     return _fn


def llava_strict_satisfaction():
    """Submits images to LLaVA and computes a reward by matching the responses to ground truth answers directly without
    using BERTScore. Prompt metadata must have "questions" and "answers" keys. See
    https://github.com/kvablack/LLaVA-server for server-side code.
    """
    import requests
    from requests.adapters import HTTPAdapter, Retry
    from io import BytesIO
    import pickle

    batch_size = 4
    url = "http://127.0.0.1:8085"
    sess = requests.Session()
    retries = Retry(
        total=1000, backoff_factor=1, status_forcelist=[500], allowed_methods=False
    )
    sess.mount("http://", HTTPAdapter(max_retries=retries))

    def _fn(images, prompts, metadata):
        del prompts
        if isinstance(images, torch.Tensor):
            images = (images * 255).round().clamp(0, 255).to(torch.uint8).cpu().numpy()
            images = images.transpose(0, 2, 3, 1)  # NCHW -> NHWC

        images_batched = np.array_split(images, np.ceil(len(images) / batch_size))
        metadata_batched = np.array_split(metadata, np.ceil(len(metadata) / batch_size))

        all_scores = []
        all_info = {
            "answers": [],
        }
        for image_batch, metadata_batch in zip(images_batched, metadata_batched):
            jpeg_images = []

            # Compress the images using JPEG
            for image in image_batch:
                img = Image.fromarray(image)
                buffer = BytesIO()
                img.save(buffer, format="JPEG", quality=80)
                jpeg_images.append(buffer.getvalue())

            # format for LLaVA server
            data = {
                "images": jpeg_images,
                "queries": [m["questions"] for m in metadata_batch],
            }
            data_bytes = pickle.dumps(data)

            # send a request to the llava server
            response = sess.post(url, data=data_bytes, timeout=120)

            response_data = pickle.loads(response.content)

            correct = np.array(
                [
                    [ans in resp for ans, resp in zip(m["answers"], responses)]
                    for m, responses in zip(metadata_batch, response_data["outputs"])
                ]
            )
            scores = correct.mean(axis=-1)

            all_scores += scores.tolist()
            all_info["answers"] += response_data["outputs"]

        return np.array(all_scores), {k: np.array(v) for k, v in all_info.items()}

    return _fn


def llava_bertscore():
    """Submits images to LLaVA and computes a reward by comparing the responses to the prompts using BERTScore. See
    https://github.com/kvablack/LLaVA-server for server-side code.
    """
    import requests
    from requests.adapters import HTTPAdapter, Retry
    from io import BytesIO
    import pickle

    batch_size = 16
    url = "http://127.0.0.1:8085"
    sess = requests.Session()
    retries = Retry(
        total=1000, backoff_factor=1, status_forcelist=[500], allowed_methods=False
    )
    sess.mount("http://", HTTPAdapter(max_retries=retries))

    def _fn(images, prompts, metadata):
        del metadata
        if isinstance(images, torch.Tensor):
            images = (images * 255).round().clamp(0, 255).to(torch.uint8).cpu().numpy()
            images = images.transpose(0, 2, 3, 1)  # NCHW -> NHWC

        images_batched = np.array_split(images, np.ceil(len(images) / batch_size))
        prompts_batched = np.array_split(prompts, np.ceil(len(prompts) / batch_size))

        all_scores = []
        all_info = {
            "precision": [],
            "f1": [],
            "outputs": [],
        }
        for image_batch, prompt_batch in zip(images_batched, prompts_batched):
            jpeg_images = []

            # Compress the images using JPEG
            for image in image_batch:
                img = Image.fromarray(image)
                buffer = BytesIO()
                img.save(buffer, format="JPEG", quality=80)
                jpeg_images.append(buffer.getvalue())

            # format for LLaVA server
            data = {
                "images": jpeg_images,
                "queries": [["Answer concisely: what is going on in this image?"]]
                * len(image_batch),
                "answers": [
                    [f"The image contains {prompt}"] for prompt in prompt_batch
                ],
            }
            data_bytes = pickle.dumps(data)

            # send a request to the llava server
            response = sess.post(url, data=data_bytes, timeout=120)

            response_data = pickle.loads(response.content)

            # use the recall score as the reward
            scores = np.array(response_data["recall"]).squeeze()
            all_scores += scores.tolist()

            # save the precision and f1 scores for analysis
            all_info["precision"] += (
                np.array(response_data["precision"]).squeeze().tolist()
            )
            all_info["f1"] += np.array(response_data["f1"]).squeeze().tolist()
            all_info["outputs"] += np.array(response_data["outputs"]).squeeze().tolist()

        return np.array(all_scores), {k: np.array(v) for k, v in all_info.items()}

    return _fn
